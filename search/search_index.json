{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"User Guide","text":"<p>Quick links:</p> <p>\ud83d\udcbb GitHub Repository \ud83d\udcda Documentation \ud83d\udc0d PyPi project \ud83e\uddea Colab Demo / Kaggle Demo</p>"},{"location":"#quickstart","title":"Quickstart","text":"<p>Install the library with pip: <pre><code>pip install dl-translate\n</code></pre></p> <p>To translate some text:</p> <pre><code>import dl_translate as dlt\n\nmt = dlt.TranslationModel()  # Slow when you load it for the first time\n\ntext_hi = \"\u0938\u0902\u092f\u0941\u0915\u094d\u0924 \u0930\u093e\u0937\u094d\u091f\u094d\u0930 \u0915\u0947 \u092a\u094d\u0930\u092e\u0941\u0916 \u0915\u093e \u0915\u0939\u0928\u093e \u0939\u0948 \u0915\u093f \u0938\u0940\u0930\u093f\u092f\u093e \u092e\u0947\u0902 \u0915\u094b\u0908 \u0938\u0948\u0928\u094d\u092f \u0938\u092e\u093e\u0927\u093e\u0928 \u0928\u0939\u0940\u0902 \u0939\u0948\"\nmt.translate(text_hi, source=dlt.lang.HINDI, target=dlt.lang.ENGLISH)\n</code></pre> <p>Above, you can see that <code>dlt.lang</code> contains variables representing each of the 50 available languages with auto-complete support. Alternatively, you can specify the language (e.g. \"Arabic\") or the language code (e.g. \"fr\" for French): <pre><code>text_ar = \"\u0627\u0644\u0623\u0645\u064a\u0646 \u0627\u0644\u0639\u0627\u0645 \u0644\u0644\u0623\u0645\u0645 \u0627\u0644\u0645\u062a\u062d\u062f\u0629 \u064a\u0642\u0648\u0644 \u0625\u0646\u0647 \u0644\u0627 \u064a\u0648\u062c\u062f \u062d\u0644 \u0639\u0633\u0643\u0631\u064a \u0641\u064a \u0633\u0648\u0631\u064a\u0627.\"\nmt.translate(text_ar, source=\"Arabic\", target=\"fr\")\n</code></pre></p> <p>If you want to verify whether a language is available, you can check it: <pre><code>print(mt.available_languages())  # All languages that you can use\nprint(mt.available_codes())  # Code corresponding to each language accepted\nprint(mt.get_lang_code_map())  # Dictionary of lang -&gt; code\n</code></pre></p>"},{"location":"#usage","title":"Usage","text":""},{"location":"#selecting-a-device","title":"Selecting a device","text":"<p>When you load the model, you can specify the device using the <code>device</code> argument. By default, the value will be <code>device=\"auto\"</code>, which means it will use a GPU if possible. You can also explicitly set <code>device=\"cpu\"</code> or <code>device=\"gpu\"</code>, or some other strings accepted by <code>torch.device()</code>. In general, it is recommend to use a GPU if you want a reasonable processing time.</p> <pre><code>mt = dlt.TranslationModel(device=\"auto\")  # Automatically select device\nmt = dlt.TranslationModel(device=\"cpu\")  # Force you to use a CPU\nmt = dlt.TranslationModel(device=\"gpu\")  # Force you to use a GPU\nmt = dlt.TranslationModel(device=\"cuda:2\")  # Use the 3rd GPU available\n</code></pre>"},{"location":"#choosing-a-different-model","title":"Choosing a different model","text":"<p>By default, the <code>m2m100</code> model will be used. However, there are a few options:</p> <ul> <li>mBART-50 Large:  Allows translations across 50 languages.</li> <li>m2m100: Allows translations across 100 languages.</li> <li>nllb-200 (New in v0.3): Allows translations across 200 languages, and is faster than m2m100 (On RTX A6000, we can see speed up of 3x).</li> </ul> <p>Here's an example: <pre><code># The default approval\nmt = dlt.TranslationModel(\"m2m100\")  # Shorthand\nmt = dlt.TranslationModel(\"facebook/m2m100_418M\")  # Huggingface repo\n\n# If you want to use mBART-50 Large\nmt = dlt.TranslationModel(\"mbart50\")\nmt = dlt.TranslationModel(\"facebook/mbart-large-50-many-to-many-mmt\")\n\n# Or NLLB-200 (faster and has 200 languages)\nmt = dlt.TranslationModel(\"nllb200\")\nmt = dlt.TranslationModel(\"facebook/nllb-200-distilled-600M\")\n</code></pre></p> <p>Note that the language code will change depending on the model family. To find out the correct language codes, please read the doc page on available languages or run <code>mt.available_codes()</code>.</p> <p>By default, <code>dlt.TranslationModel</code> will download the model from the huggingface repo for mbart50, m2m100, or nllb200 and cache it. It's possible to load the model from a path or a model with a similar format, but you will need to specify the <code>model_family</code>: <pre><code>mt = dlt.TranslationModel(\"/path/to/model/directory/\", model_family=\"mbart50\")\nmt = dlt.TranslationModel(\"facebook/m2m100_1.2B\", model_family=\"m2m100\")\nmt = dlt.TranslationModel(\"facebook/nllb-200-distilled-600M\", model_family=\"nllb200\")\n</code></pre></p> <p>Notes: * Make sure your tokenizer is also stored in the same directory if you load from a file.  * The available languages will change if you select a different model, so you will not be able to leverage <code>dlt.lang</code> or <code>dlt.utils</code>.</p>"},{"location":"#breaking-down-into-sentences","title":"Breaking down into sentences","text":"<p>It is not recommended to use extremely long texts as it takes more time to process. Instead, you can try to break them down into sentences. Multiple solutions exists for that, including doing it manually and using the <code>nltk</code> library.</p> <p>A quick approach would be to split them by period. However, you have to ensure that there are no periods used for abbreviations (such as <code>Mr.</code> or <code>Dr.</code>). For example, it will work in the following case: <pre><code>text = \"Mr Smith went to his favorite cafe. There, he met his friend Dr Doe.\"\nsents = text.split(\".\")\n\".\".join(mt.translate(sents, source=dlt.lang.ENGLISH, target=dlt.lang.FRENCH))\n</code></pre></p> <p>For more complex cases (e.g. where you use periods for abbreviations), you can use <code>nltk</code>. First install the library with <code>pip install nltk</code>, then run: <pre><code>import nltk\n\nnltk.download(\"punkt\")\n\ntext = \"Mr. Smith went to his favorite cafe. There, he met his friend Dr. Doe.\"\nsents = nltk.tokenize.sent_tokenize(text, \"english\")  # don't use dlt.lang.ENGLISH\n\" \".join(mt.translate(sents, source=dlt.lang.ENGLISH, target=dlt.lang.FRENCH))\n</code></pre></p>"},{"location":"#batch-size-and-verbosity-when-using-translate","title":"Batch size and verbosity when using <code>translate</code>","text":"<p>It's possible to set a batch size (i.e. the number of elements processed at once) for <code>mt.translate</code> and whether you want to see the progress bar or not:</p> <pre><code>...\nmt = dlt.TranslationModel()\nmt.translate(text, source, target, batch_size=32, verbose=True)\n</code></pre> <p>If you set <code>batch_size=None</code>, it will compute the entire <code>text</code> at once rather than splitting into \"chunks\". We recommend lowering <code>batch_size</code> if you do not have a lot of RAM or VRAM and run into CUDA memory error. Set a higher value if you are using a high-end GPU and the VRAM is not fully utilized.</p>"},{"location":"#dltutils-module","title":"<code>dlt.utils</code> module","text":"<p>An alternative to <code>mt.available_languages()</code> is the <code>dlt.utils</code> module. You can use it to find out which languages and codes are available:</p> <pre><code>print(dlt.utils.available_languages('mbart50'))  # All languages that you can use\nprint(dlt.utils.available_codes('mbart50'))  # Code corresponding to each language accepted\nprint(dlt.utils.get_lang_code_map('mbart50'))  # Dictionary of lang -&gt; code\nprint(dlt.utils.available_languages('m2m100'))  # write the name of the model family\n</code></pre> <p>At the moment, the following models are accepted: - <code>\"mbart50\"</code> - <code>\"m2m100\"</code> - <code>\"nllb200\"</code></p>"},{"location":"#offline-usage","title":"Offline usage","text":"<p>Unlike the Google translate or MSFT Translator APIs, this library can be fully used offline. However, you will need to first download the packages and models, and move them to your offline environment to be installed and loaded inside a venv.</p> <p>First, run in your terminal: <pre><code>mkdir dlt\ncd dlt\nmkdir libraries\npip download -d libraries/ dl-translate\n</code></pre></p> <p>Once all the required packages are downloaded, you will need to use huggingface hub to download the files. Install it with <code>pip install huggingface-hub</code>. Then, run inside Python: <pre><code>import shutil\nimport huggingface_hub as hub\n\ndirname = hub.snapshot_download(\"facebook/m2m100_418M\")\nshutil.copytree(dirname, \"cached_model_m2m100\")  # Copy to a permanent folder\n</code></pre></p> <p>Now, move everything in the <code>dlt</code> directory to your offline environment. Create a virtual environment and run the following in terminal: <pre><code>pip install --no-index --find-links libraries/ dl-translate\n</code></pre></p> <p>Now, run inside Python: <pre><code>import dl_translate as dlt\n\nmt = dlt.TranslationModel(\"cached_model_m2m100\", model_family=\"m2m100\")\n</code></pre></p>"},{"location":"#advanced","title":"Advanced","text":"<p>The following section assumes you have knowledge of PyTorch and Huggingface Transformers.</p>"},{"location":"#saving-and-loading","title":"Saving and loading","text":"<p>If you wish to accelerate the loading time the translation model, you can use <code>save_obj</code>. Later you can reload it with <code>load_obj</code> by specifying the same directory that you are using to save.</p> <pre><code>mt = dlt.TranslationModel()\n# ...\nmt.save_obj('saved_model')\n# ...\nmt = dlt.TranslationModel.load_obj('saved_model')\n</code></pre> <p>Warning: Only use this if you are certain the torch module saved in <code>saved_model/weights.pt</code> can be correctly loaded. Indeed, it is possible that the <code>huggingface</code>, <code>torch</code> or some other dependencies change between when you called <code>save_obj</code> and <code>load_obj</code>, and that might break your code. Thus, it is recommend to only run <code>load_obj</code> in the same environment/session as <code>save_obj</code>. Note this method might be deprecated in the future once there's no speed benefit in loading this way.</p>"},{"location":"#interacting-with-underlying-model-and-tokenizer","title":"Interacting with underlying model and tokenizer","text":"<p>When initializing <code>model</code>, you can pass in arguments for the underlying BART model and tokenizer (which will respectively be passed to <code>ModelForConditionalGeneration.from_pretrained</code> and <code>TokenizerFast.from_pretrained</code>):</p> <pre><code>mt = dlt.TranslationModel(\n    model_options=dict(\n        state_dict=...,\n        cache_dir=...,\n        ...\n    ),\n    tokenizer_options=dict(\n        tokenizer_file=...,\n        eos_token=...,\n        ...\n    )\n)\n</code></pre> <p>You can also access the underlying <code>transformers</code> model and <code>tokenizer</code>: <pre><code>transformers_model = mt.get_transformers_model()\ntokenizer = mt.get_tokenizer()\n</code></pre></p> <p>For more information about the models themselves, please read the docs on mBART and m2m100.</p>"},{"location":"#keyword-arguments-for-the-generate-method-of-the-underlying-model","title":"Keyword arguments for the <code>generate()</code> method of the underlying model","text":"<p>When running <code>mt.translate</code>, you can also give a <code>generation_options</code> dictionary that is passed as keyword arguments to the underlying <code>mt.get_transformers_model().generate()</code> method: <pre><code>mt.translate(\n    text,\n    source=dlt.lang.GERMAN,\n    target=dlt.lang.SPANISH,\n    generation_options=dict(num_beams=5, max_length=...)\n)\n</code></pre></p> <p>Learn more in the huggingface docs.</p>"},{"location":"available_languages/","title":"Languages Available","text":"<p>This page gives all the languages available for each model family.</p>"},{"location":"available_languages/#mbart-50","title":"MBart 50","text":"Language Name Code Arabic ar_AR Czech cs_CZ German de_DE English en_XX Spanish es_XX Estonian et_EE Finnish fi_FI French fr_XX Gujarati gu_IN Hindi hi_IN Italian it_IT Japanese ja_XX Kazakh kk_KZ Korean ko_KR Lithuanian lt_LT Latvian lv_LV Burmese my_MM Nepali ne_NP Dutch nl_XX Romanian ro_RO Russian ru_RU Sinhala si_LK Turkish tr_TR Vietnamese vi_VN Chinese zh_CN Afrikaans af_ZA Azerbaijani az_AZ Bengali bn_IN Persian fa_IR Hebrew he_IL Croatian hr_HR Indonesian id_ID Georgian ka_GE Khmer km_KH Macedonian mk_MK Malayalam ml_IN Mongolian mn_MN Marathi mr_IN Polish pl_PL Pashto ps_AF Portuguese pt_XX Swedish sv_SE Swahili sw_KE Tamil ta_IN Telugu te_IN Thai th_TH Tagalog tl_XX Ukrainian uk_UA Urdu ur_PK Xhosa xh_ZA Galician gl_ES Slovene sl_SI"},{"location":"available_languages/#m2m-100","title":"M2M-100","text":"Language Name Code Afrikaans af Amharic am Arabic ar Asturian ast Azerbaijani az Bashkir ba Belarusian be Bulgarian bg Bengali bn Breton br Bosnian bs Catalan ca Valencian ca Cebuano ceb Czech cs Welsh cy Danish da German de Greek el English en Spanish es Estonian et Persian fa Fulah ff Finnish fi French fr Western Frisian fy Irish ga Gaelic gd Scottish Gaelic gd Galician gl Gujarati gu Hausa ha Hebrew he Hindi hi Croatian hr Haitian ht Haitian Creole ht Hungarian hu Armenian hy Indonesian id Igbo ig Iloko ilo Icelandic is Italian it Japanese ja Javanese jv Georgian ka Kazakh kk Khmer km Central Khmer km Kannada kn Korean ko Luxembourgish lb Letzeburgesch lb Ganda lg Lingala ln Lao lo Lithuanian lt Latvian lv Malagasy mg Macedonian mk Malayalam ml Mongolian mn Marathi mr Malay ms Burmese my Nepali ne Dutch nl Flemish nl Norwegian no Northern Sotho ns Occitan oc Oriya or Panjabi pa Punjabi pa Polish pl Pushto ps Pashto ps Portuguese pt Romanian ro Moldavian ro Moldovan ro Russian ru Sindhi sd Sinhala si Sinhalese si Slovak sk Slovenian sl Somali so Albanian sq Serbian sr Swati ss Sundanese su Swedish sv Swahili sw Tamil ta Thai th Tagalog tl Tswana tn Turkish tr Ukrainian uk Urdu ur Uzbek uz Vietnamese vi Wolof wo Xhosa xh Yiddish yi Yoruba yo Chinese zh Zulu zu"},{"location":"available_languages/#nllb-200","title":"NLLB-200","text":"Language Name Code Acehnese (Arabic script) ace_Arab Acehnese (Latin script) ace_Latn Mesopotamian Arabic acm_Arab Ta'izzi-Adeni Arabic acq_Arab Tunisian Arabic aeb_Arab Afrikaans afr_Latn South Levantine Arabic ajp_Arab Akan aka_Latn Amharic amh_Ethi North Levantine Arabic apc_Arab Modern Standard Arabic arb_Arab Modern Standard Arabic (Romanized) arb_Latn Najdi Arabic ars_Arab Moroccan Arabic ary_Arab Egyptian Arabic arz_Arab Assamese asm_Beng Asturian ast_Latn Awadhi awa_Deva Central Aymara ayr_Latn South Azerbaijani azb_Arab North Azerbaijani azj_Latn Bashkir bak_Cyrl Bambara bam_Latn Balinese ban_Latn Belarusian bel_Cyrl Bemba bem_Latn Bengali ben_Beng Bhojpuri bho_Deva Banjar (Arabic script) bjn_Arab Banjar (Latin script) bjn_Latn Standard Tibetan bod_Tibt Bosnian bos_Latn Buginese bug_Latn Bulgarian bul_Cyrl Catalan cat_Latn Cebuano ceb_Latn Czech ces_Latn Chokwe cjk_Latn Central Kurdish ckb_Arab Crimean Tatar crh_Latn Welsh cym_Latn Danish dan_Latn German deu_Latn Southwestern Dinka dik_Latn Dyula dyu_Latn Dzongkha dzo_Tibt Greek ell_Grek English eng_Latn Esperanto epo_Latn Estonian est_Latn Basque eus_Latn Ewe ewe_Latn Faroese fao_Latn Fijian fij_Latn Finnish fin_Latn Fon fon_Latn French fra_Latn Friulian fur_Latn Nigerian Fulfulde fuv_Latn Scottish Gaelic gla_Latn Irish gle_Latn Galician glg_Latn Guarani grn_Latn Gujarati guj_Gujr Haitian Creole hat_Latn Hausa hau_Latn Hebrew heb_Hebr Hindi hin_Deva Chhattisgarhi hne_Deva Croatian hrv_Latn Hungarian hun_Latn Armenian hye_Armn Igbo ibo_Latn Ilocano ilo_Latn Indonesian ind_Latn Icelandic isl_Latn Italian ita_Latn Javanese jav_Latn Japanese jpn_Jpan Kabyle kab_Latn Jingpho kac_Latn Kamba kam_Latn Kannada kan_Knda Kashmiri (Arabic script) kas_Arab Kashmiri (Devanagari script) kas_Deva Georgian kat_Geor Central Kanuri (Arabic script) knc_Arab Central Kanuri (Latin script) knc_Latn Kazakh kaz_Cyrl Kabiy\u00e8 kbp_Latn Kabuverdianu kea_Latn Khmer khm_Khmr Kikuyu kik_Latn Kinyarwanda kin_Latn Kyrgyz kir_Cyrl Kimbundu kmb_Latn Northern Kurdish kmr_Latn Kikongo kon_Latn Korean kor_Hang Lao lao_Laoo Ligurian lij_Latn Limburgish lim_Latn Lingala lin_Latn Lithuanian lit_Latn Lombard lmo_Latn Latgalian ltg_Latn Luxembourgish ltz_Latn Luba-Kasai lua_Latn Ganda lug_Latn Luo luo_Latn Mizo lus_Latn Standard Latvian lvs_Latn Magahi mag_Deva Maithili mai_Deva Malayalam mal_Mlym Marathi mar_Deva Minangkabau (Arabic script) min_Arab Minangkabau (Latin script) min_Latn Macedonian mkd_Cyrl Plateau Malagasy plt_Latn Maltese mlt_Latn Meitei (Bengali script) mni_Beng Halh Mongolian khk_Cyrl Mossi mos_Latn Maori mri_Latn Burmese mya_Mymr Dutch nld_Latn Norwegian Nynorsk nno_Latn Norwegian Bokm\u00e5l nob_Latn Nepali npi_Deva Northern Sotho nso_Latn Nuer nus_Latn Nyanja nya_Latn Occitan oci_Latn West Central Oromo gaz_Latn Odia ory_Orya Pangasinan pag_Latn Eastern Panjabi pan_Guru Papiamento pap_Latn Western Persian pes_Arab Polish pol_Latn Portuguese por_Latn Dari prs_Arab Southern Pashto pbt_Arab Ayacucho Quechua quy_Latn Romanian ron_Latn Rundi run_Latn Russian rus_Cyrl Sango sag_Latn Sanskrit san_Deva Santali sat_Olck Sicilian scn_Latn Shan shn_Mymr Sinhala sin_Sinh Slovak slk_Latn Slovenian slv_Latn Samoan smo_Latn Shona sna_Latn Sindhi snd_Arab Somali som_Latn Southern Sotho sot_Latn Spanish spa_Latn Tosk Albanian als_Latn Sardinian srd_Latn Serbian srp_Cyrl Swati ssw_Latn Sundanese sun_Latn Swedish swe_Latn Swahili swh_Latn Silesian szl_Latn Tamil tam_Taml Tatar tat_Cyrl Telugu tel_Telu Tajik tgk_Cyrl Tagalog tgl_Latn Thai tha_Thai Tigrinya tir_Ethi Tamasheq (Latin script) taq_Latn Tamasheq (Tifinagh script) taq_Tfng Tok Pisin tpi_Latn Tswana tsn_Latn Tsonga tso_Latn Turkmen tuk_Latn Tumbuka tum_Latn Turkish tur_Latn Twi twi_Latn Central Atlas Tamazight tzm_Tfng Uyghur uig_Arab Ukrainian ukr_Cyrl Umbundu umb_Latn Urdu urd_Arab Northern Uzbek uzn_Latn Venetian vec_Latn Vietnamese vie_Latn Waray war_Latn Wolof wol_Latn Xhosa xho_Latn Eastern Yiddish ydd_Hebr Yoruba yor_Latn Yue Chinese yue_Hant Chinese (Simplified) zho_Hans Chinese (Traditional) zho_Hant Standard Malay zsm_Latn Zulu zul_Latn"},{"location":"contributing/","title":"Contributions","text":"<p>If you wish to contribute to the project, please do the following: 1. Verify if there's an existing similar issue. 2. If no issue exists, create it. 3. Once the contribution has been discussed inside the issue, fork this repo. 4. Before modifying any code, make sure to read the sections below. 5. Once you are done with your contribution, start a PR and tag a codeowner.</p>"},{"location":"contributing/#setup","title":"Setup","text":"<p>To set up the development environment, clone the repo:</p> <pre><code>git clone https://github.com/xhlulu/dl-translate\ncd dl-translate\n</code></pre> <p>Create a new venv and install the dev dependencies <pre><code>python -m venv venv\nsource venv/bin/activate\npip install -e .[dev]\n</code></pre></p>"},{"location":"contributing/#code-linting","title":"Code linting","text":"<p>To ensure consistent and readable code, we use <code>black</code>. To run:</p> <pre><code>python black .\n</code></pre>"},{"location":"contributing/#running-tests","title":"Running tests","text":"<p>To run all the tests: <pre><code>python -m pytest tests\n</code></pre></p> <p>For quick tests, run: <pre><code>python -m pytest tests/fast\n</code></pre></p>"},{"location":"contributing/#documentation","title":"Documentation","text":"<p>To re-generate the documentation after the source code was modified: <pre><code>python scripts/render_references.py\n</code></pre></p> <p>To run the docs locally, run: <pre><code>mkdocs serve -t material\n</code></pre></p> <p>Once ready, you can build it: <pre><code>mkdocs build -t material\n</code></pre></p> <p>Or release it on GitHub Pages: <pre><code>mkdocs gh-deploy -t material\n</code></pre></p>"},{"location":"references/","title":"API Reference","text":""},{"location":"references/#dlttranslationmodel","title":"dlt.TranslationModel","text":""},{"location":"references/#init","title":"init","text":"<pre><code>dlt.TranslationModel.__init__(self, model_or_path: str = 'm2m100', tokenizer_path: str = None, device: str = 'auto', model_family: str = None, model_options: dict = None, tokenizer_options: dict = None)\n</code></pre> <p>Instantiates a multilingual transformer model for translation.</p> Parameter Type Default Description model_or_path str <code>m2m100</code> The path or the name of the model. Equivalent to the first argument of <code>AutoModel.from_pretrained()</code>. You can also specify shorthands (\"mbart50\" and \"m2m100\"). tokenizer_path str optional The path to the tokenizer. By default, it will be set to <code>model_or_path</code>. device str <code>auto</code> \"cpu\", \"gpu\" or \"auto\". If it's set to \"auto\", will try to select a GPU when available or else fall back to CPU. model_family str optional Either \"mbart50\" or \"m2m100\". By default, it will be inferred based on <code>model_or_path</code>. Needs to be explicitly set if <code>model_or_path</code> is a path. model_options dict optional The keyword arguments passed to the model, which is a transformer for conditional generation. tokenizer_options dict optional The keyword arguments passed to the model's tokenizer. <p></p>"},{"location":"references/#translate","title":"translate","text":"<pre><code>dlt.TranslationModel.translate(self, text: Union[str, List[str]], source: str, target: str, batch_size: int = 32, verbose: bool = False, generation_options: dict = None) -&gt; Union[str, List[str]]\n</code></pre> <p>Translates a string or a list of strings from a source to a target language.</p> Parameter Type Default Description text Union[str, List[str]] required The content you want to translate. source str required The language of the original text. target str required The language of the translated text. batch_size int <code>32</code> The number of samples to load at once. If set to <code>None</code>, it will process everything at once. verbose bool <code>False</code> Whether to display the progress bar for every batch processed. generation_options dict optional The keyword arguments passed to <code>model.generate()</code>, where <code>model</code> is the underlying transformers model. <p>Note: - Run <code>print(dlt.utils.available_languages())</code> to see what's available. - A smaller value is preferred for <code>batch_size</code> if your (video) RAM is limited.</p> <p></p>"},{"location":"references/#get_transformers_model","title":"get_transformers_model","text":"<pre><code>dlt.TranslationModel.get_transformers_model(self)\n</code></pre> <p>Retrieve the underlying mBART transformer model.</p> <p></p>"},{"location":"references/#get_tokenizer","title":"get_tokenizer","text":"<pre><code>dlt.TranslationModel.get_tokenizer(self)\n</code></pre> <p>Retrieve the mBART huggingface tokenizer.</p> <p></p>"},{"location":"references/#available_codes","title":"available_codes","text":"<pre><code>dlt.TranslationModel.available_codes(self) -&gt; List[str]\n</code></pre> <p>Returns all the available codes for a given <code>dlt.TranslationModel</code> instance.</p> <p></p>"},{"location":"references/#available_languages","title":"available_languages","text":"<pre><code>dlt.TranslationModel.available_languages(self) -&gt; List[str]\n</code></pre> <p>Returns all the available languages for a given <code>dlt.TranslationModel</code> instance.</p> <p></p>"},{"location":"references/#get_lang_code_map","title":"get_lang_code_map","text":"<pre><code>dlt.TranslationModel.get_lang_code_map(self) -&gt; Dict[str, str]\n</code></pre> <p>Returns the language -&gt; codes dictionary for a given <code>dlt.TranslationModel</code> instance.</p> <p></p>"},{"location":"references/#save_obj","title":"save_obj","text":"<pre><code>dlt.TranslationModel.save_obj(self, path: str = 'saved_model') -&gt; None\n</code></pre> <p>Saves your model as a torch object and save your tokenizer.</p> Parameter Type Default Description path str <code>saved_model</code> The directory where you want to save your model and tokenizer <p></p>"},{"location":"references/#load_obj","title":"load_obj","text":"<pre><code>dlt.TranslationModel.load_obj(path: str = 'saved_model', **kwargs)\n</code></pre> <p>Initialize <code>dlt.TranslationModel</code> from the torch object and tokenizer saved with <code>dlt.TranslationModel.save_obj</code></p> Parameter Type Default Description path str <code>saved_model</code> The directory where your torch model and tokenizer are stored <p></p> <p></p>"},{"location":"references/#dltutils","title":"dlt.utils","text":""},{"location":"references/#get_lang_code_map_1","title":"get_lang_code_map","text":"<pre><code>dlt.utils.get_lang_code_map(weights: str = 'mbart50') -&gt; Dict[str, str]\n</code></pre> <p>Get a dictionary mapping a language -&gt; code for a given model. The code will depend on the model you choose.</p> Parameter Type Default Description weights str <code>mbart50</code> The name of the model you are using. For example, \"mbart50\" is the multilingual BART Large with 50 languages available to use. <p></p>"},{"location":"references/#available_codes_1","title":"available_codes","text":"<pre><code>dlt.utils.available_codes(weights: str = 'mbart50') -&gt; List[str]\n</code></pre> <p>Get all the codes available for a given model. The code format will depend on the model you select.</p> Parameter Type Default Description weights str <code>mbart50</code> The name of the model you are using. For example, \"mbart50\" is the multilingual BART Large with 50 codes available to use. <p></p>"},{"location":"references/#available_languages_1","title":"available_languages","text":"<pre><code>dlt.utils.available_languages(weights: str = 'mbart50') -&gt; List[str]\n</code></pre> <p>Get all the languages available for a given model.</p> Parameter Type Default Description weights str <code>mbart50</code> The name of the model you are using. For example, \"mbart50\" is the multilingual BART Large with 50 languages available to use. <p></p> <p></p>"}]}
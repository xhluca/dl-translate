{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.9",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "name": "dl-translate demo.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "9eac029aa8a34135b1e619837a0d501f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_1992924072de4a6d8db0fea6094274ae",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_cc7c647ebc3342eca5fcc1c027dd0a7d",
              "IPY_MODEL_c75bc40d75c744e0b8518ebbe4f4cb00"
            ]
          }
        },
        "1992924072de4a6d8db0fea6094274ae": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "cc7c647ebc3342eca5fcc1c027dd0a7d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_817e672c133e484fa8a12fa0130b3336",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_b25f76f8822d4ae8965a3950a28e651e"
          }
        },
        "c75bc40d75c744e0b8518ebbe4f4cb00": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_44bffa64c51b4752ac983d08ea8f00f3",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "‚Äã",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1/1 [00:03&lt;00:00,  3.89s/it]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_6d1c65c8195f4f26895bfb3a9092fba4"
          }
        },
        "817e672c133e484fa8a12fa0130b3336": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "b25f76f8822d4ae8965a3950a28e651e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "44bffa64c51b4752ac983d08ea8f00f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "6d1c65c8195f4f26895bfb3a9092fba4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tx6xJha5YIiA"
      },
      "source": [
        "# DL Translate\n",
        "\n",
        "*A deep learning-based translation library built on Huggingface `transformers` and Facebook's `mBART-Large`*\n",
        "\n",
        "üíª [GitHub Repository](https://github.com/xhlulu/dl-translate)\\\n",
        "üìö [Documentation](https://git.io/dlt-docs) / [readthedocs](https://dl-translate.readthedocs.io)\\\n",
        "üêç [PyPi project](https://pypi.org/project/dl-translate/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bCjxVhyxYIiD"
      },
      "source": [
        "## Quickstart\n",
        "\n",
        "Install the library with pip:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "_kg_hide-input": false,
        "_kg_hide-output": true,
        "id": "BI3mAoRnYIiF"
      },
      "source": [
        "!pip install -q dl-translate"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p1oeb4czYIiG"
      },
      "source": [
        "To translate some text:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "qdefSjR_YIiG",
        "outputId": "b0c73f2a-2f83-4cd6-ab79-3b34606ad634"
      },
      "source": [
        "import dl_translate as dlt\n",
        "\n",
        "mt = dlt.TranslationModel()\n",
        "\n",
        "text_hi = \"‡§∏‡§Ç‡§Ø‡•Å‡§ï‡•ç‡§§ ‡§∞‡§æ‡§∑‡•ç‡§ü‡•ç‡§∞ ‡§ï‡•á ‡§™‡•ç‡§∞‡§Æ‡•Å‡§ñ ‡§ï‡§æ ‡§ï‡§π‡§®‡§æ ‡§π‡•à ‡§ï‡§ø ‡§∏‡•Ä‡§∞‡§ø‡§Ø‡§æ ‡§Æ‡•á‡§Ç ‡§ï‡•ã‡§à ‡§∏‡•à‡§®‡•ç‡§Ø ‡§∏‡§Æ‡§æ‡§ß‡§æ‡§® ‡§®‡§π‡•Ä‡§Ç ‡§π‡•à\"\n",
        "mt.translate(text_hi, source=dlt.lang.HINDI, target=dlt.lang.ENGLISH)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The head of the United Nations says there is no military solution in Syria'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DDQGpznwYIiH"
      },
      "source": [
        "Above, you can see that `dlt.lang` contains variables representing each of the 50 available languages with auto-complete support. Alternatively, you can specify the language (e.g. \"Arabic\") or the language code (e.g. \"fr_XX\" for French):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "yC3LMjmNYIiI",
        "outputId": "53f2172c-e7f9-44fb-f6da-702f7070310c"
      },
      "source": [
        "text_ar = \"ÿßŸÑÿ£ŸÖŸäŸÜ ÿßŸÑÿπÿßŸÖ ŸÑŸÑÿ£ŸÖŸÖ ÿßŸÑŸÖÿ™ÿ≠ÿØÿ© ŸäŸÇŸàŸÑ ÿ•ŸÜŸá ŸÑÿß ŸäŸàÿ¨ÿØ ÿ≠ŸÑ ÿπÿ≥ŸÉÿ±Ÿä ŸÅŸä ÿ≥Ÿàÿ±Ÿäÿß.\"\n",
        "mt.translate(text_ar, source=\"Arabic\", target=\"fr_XX\")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"Le Secr√©taire g√©n√©ral de l'ONU dit qu'il n'y a pas de solution militaire en Syrie.\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "erptbvbiYIiI"
      },
      "source": [
        "If you want to verify whether a language is available, you can check it:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_kg_hide-output": false,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "saHalYvsYIiJ",
        "outputId": "3d675f9f-384b-43c0-e3a5-7cea7598eae7"
      },
      "source": [
        "print(mt.available_languages())  # All languages that you can use\n",
        "print(mt.available_codes())  # Code corresponding to each language accepted\n",
        "print(mt.get_lang_code_map())  # Dictionary of lang -> code"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('Arabic', 'Czech', 'German', 'English', 'Spanish', 'Estonian', 'Finnish', 'French', 'Gujarati', 'Hindi', 'Italian', 'Japanese', 'Kazakh', 'Korean', 'Lithuanian', 'Latvian', 'Burmese', 'Nepali', 'Dutch', 'Romanian', 'Russian', 'Sinhala', 'Turkish', 'Vietnamese', 'Chinese', 'Afrikaans', 'Azerbaijani', 'Bengali', 'Persian', 'Hebrew', 'Croatian', 'Indonesian', 'Georgian', 'Khmer', 'Macedonian', 'Malayalam', 'Mongolian', 'Marathi', 'Polish', 'Pashto', 'Portuguese', 'Swedish', 'Swahili', 'Tamil', 'Telugu', 'Thai', 'Tagalog', 'Ukrainian', 'Urdu', 'Xhosa', 'Galician', 'Slovene')\n",
            "('Arabic', 'Czech', 'German', 'English', 'Spanish', 'Estonian', 'Finnish', 'French', 'Gujarati', 'Hindi', 'Italian', 'Japanese', 'Kazakh', 'Korean', 'Lithuanian', 'Latvian', 'Burmese', 'Nepali', 'Dutch', 'Romanian', 'Russian', 'Sinhala', 'Turkish', 'Vietnamese', 'Chinese', 'Afrikaans', 'Azerbaijani', 'Bengali', 'Persian', 'Hebrew', 'Croatian', 'Indonesian', 'Georgian', 'Khmer', 'Macedonian', 'Malayalam', 'Mongolian', 'Marathi', 'Polish', 'Pashto', 'Portuguese', 'Swedish', 'Swahili', 'Tamil', 'Telugu', 'Thai', 'Tagalog', 'Ukrainian', 'Urdu', 'Xhosa', 'Galician', 'Slovene')\n",
            "{'Arabic': 'ar_AR', 'Czech': 'cs_CZ', 'German': 'de_DE', 'English': 'en_XX', 'Spanish': 'es_XX', 'Estonian': 'et_EE', 'Finnish': 'fi_FI', 'French': 'fr_XX', 'Gujarati': 'gu_IN', 'Hindi': 'hi_IN', 'Italian': 'it_IT', 'Japanese': 'ja_XX', 'Kazakh': 'kk_KZ', 'Korean': 'ko_KR', 'Lithuanian': 'lt_LT', 'Latvian': 'lv_LV', 'Burmese': 'my_MM', 'Nepali': 'ne_NP', 'Dutch': 'nl_XX', 'Romanian': 'ro_RO', 'Russian': 'ru_RU', 'Sinhala': 'si_LK', 'Turkish': 'tr_TR', 'Vietnamese': 'vi_VN', 'Chinese': 'zh_CN', 'Afrikaans': 'af_ZA', 'Azerbaijani': 'az_AZ', 'Bengali': 'bn_IN', 'Persian': 'fa_IR', 'Hebrew': 'he_IL', 'Croatian': 'hr_HR', 'Indonesian': 'id_ID', 'Georgian': 'ka_GE', 'Khmer': 'km_KH', 'Macedonian': 'mk_MK', 'Malayalam': 'ml_IN', 'Mongolian': 'mn_MN', 'Marathi': 'mr_IN', 'Polish': 'pl_PL', 'Pashto': 'ps_AF', 'Portuguese': 'pt_XX', 'Swedish': 'sv_SE', 'Swahili': 'sw_KE', 'Tamil': 'ta_IN', 'Telugu': 'te_IN', 'Thai': 'th_TH', 'Tagalog': 'tl_XX', 'Ukrainian': 'uk_UA', 'Urdu': 'ur_PK', 'Xhosa': 'xh_ZA', 'Galician': 'gl_ES', 'Slovene': 'sl_SI'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "trusted": true,
        "id": "lz3Rq5t0YIiJ"
      },
      "source": [
        "## Usage\n",
        "\n",
        "### Selecting a device\n",
        "\n",
        "When you load the model, you can specify the device:\n",
        "```python\n",
        "mt = dlt.TranslationModel(device=\"auto\")\n",
        "```\n",
        "\n",
        "By default, the value will be `device=\"auto\"`, which means it will use a GPU if possible. You can also explicitly set `device=\"cpu\"` or `device=\"gpu\"`, or some other strings accepted by [`torch.device()`](https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.device). __In general, it is recommend to use a GPU if you want a reasonable processing time.__\n",
        "\n",
        "Let's check what we originally loaded:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fKAlEmzbYIiJ",
        "outputId": "407d9e3e-fa36-4074-da67-a51f9533298a"
      },
      "source": [
        "mt.device"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nm7hVlSZYIiL"
      },
      "source": [
        "### Loading from a path\n",
        "\n",
        "By default, `dlt.TranslationModel` will download the model from the [huggingface repo](https://huggingface.co/facebook/mbart-large-50-one-to-many-mmt) and cache it. However, you are free to load from a path:\n",
        "```python\n",
        "mt = dlt.TranslationModel(\"/path/to/your/model/directory/\")\n",
        "```\n",
        "Make sure that your tokenizer is also stored in the same directory if you use this approach.\n",
        "\n",
        "\n",
        "### Using a different model\n",
        "\n",
        "You can also choose another model that has [a similar format](https://huggingface.co/models?filter=mbart-50), e.g.\n",
        "```python\n",
        "mt = dlt.TranslationModel(\"facebook/mbart-large-50-one-to-many-mmt\")\n",
        "```\n",
        "Note that the available languages will change if you do this, so you will not be able to leverage `dlt.lang` or `dlt.utils`.\n",
        "\n",
        "\n",
        "### Breaking down into sentences\n",
        "\n",
        "It is not recommended to use extremely long texts as it takes more time to process. Instead, you can try to break them down into sentences with the help of `nltk`. First install the library with `pip install nltk`, then run:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_kg_hide-output": true,
        "trusted": true,
        "id": "XkjrydidYIiL"
      },
      "source": [
        "import nltk\n",
        "nltk.download(\"punkt\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "j-cyjxQCYIiL",
        "outputId": "9fbbff37-715a-435e-ffef-61e77bb8f859"
      },
      "source": [
        "text = \"Mr. Smith went to his favorite cafe. There, he met his friend Dr. Doe.\"\n",
        "sents = nltk.tokenize.sent_tokenize(text, \"english\")  # don't use dlt.lang.ENGLISH\n",
        "\" \".join(mt.translate(sents, source=dlt.lang.ENGLISH, target=dlt.lang.FRENCH))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"M. Smith s'est rendu dans son caf√© pr√©f√©r√©. L√†, il a rencontr√© son ami, le Dr Doe.\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u6y8-SthYIiM"
      },
      "source": [
        "### Setting a `batch_size` and verbosity when calling `dlt.TranslationModel.translate`\n",
        "\n",
        "It's possible to set a batch size (i.e. the number of elements processed at once) for `mt.translate` and whether you want to see the progress bar or not:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104,
          "referenced_widgets": [
            "9eac029aa8a34135b1e619837a0d501f",
            "1992924072de4a6d8db0fea6094274ae",
            "cc7c647ebc3342eca5fcc1c027dd0a7d",
            "c75bc40d75c744e0b8518ebbe4f4cb00",
            "817e672c133e484fa8a12fa0130b3336",
            "b25f76f8822d4ae8965a3950a28e651e",
            "44bffa64c51b4752ac983d08ea8f00f3",
            "6d1c65c8195f4f26895bfb3a9092fba4"
          ]
        },
        "id": "fcxUFmjAYIiM",
        "outputId": "eae8301e-0d8c-431b-edd8-581b2f50ac0f"
      },
      "source": [
        "mt.translate(sents, source=dlt.lang.ENGLISH, target=dlt.lang.FRENCH, batch_size=32, verbose=True)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9eac029aa8a34135b1e619837a0d501f",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"M. Smith s'est rendu dans son caf√© pr√©f√©r√©.\",\n",
              " 'L√†, il a rencontr√© son ami, le Dr Doe.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iS0bfSh_YIiM"
      },
      "source": [
        "If you set `batch_size=None`, it will compute the entire `text` at once rather than splitting into \"chunks\". We recommend lowering `batch_size` if you do not have a lot of RAM or VRAM and run into CUDA memory error. Set a higher value if you are using a high-end GPU and the VRAM is not fully utilized.\n",
        "\n",
        "\n",
        "### `dlt.utils` module\n",
        "\n",
        "An alternative to `mt.available_languages()` is the `dlt.utils` module. You can use it to find out which languages and codes are available:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U7iS_wKTYIiM",
        "outputId": "123717b1-0661-4a5b-ed6e-01ef680fc7cf"
      },
      "source": [
        "print(dlt.utils.available_languages('mbart50'))  # All languages that you can use\n",
        "print(dlt.utils.available_codes('mbart50'))  # Code corresponding to each language accepted\n",
        "print(dlt.utils.get_lang_code_map('mbart50'))  # Dictionary of lang -> code"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('Arabic', 'Czech', 'German', 'English', 'Spanish', 'Estonian', 'Finnish', 'French', 'Gujarati', 'Hindi', 'Italian', 'Japanese', 'Kazakh', 'Korean', 'Lithuanian', 'Latvian', 'Burmese', 'Nepali', 'Dutch', 'Romanian', 'Russian', 'Sinhala', 'Turkish', 'Vietnamese', 'Chinese', 'Afrikaans', 'Azerbaijani', 'Bengali', 'Persian', 'Hebrew', 'Croatian', 'Indonesian', 'Georgian', 'Khmer', 'Macedonian', 'Malayalam', 'Mongolian', 'Marathi', 'Polish', 'Pashto', 'Portuguese', 'Swedish', 'Swahili', 'Tamil', 'Telugu', 'Thai', 'Tagalog', 'Ukrainian', 'Urdu', 'Xhosa', 'Galician', 'Slovene')\n",
            "('ar_AR', 'cs_CZ', 'de_DE', 'en_XX', 'es_XX', 'et_EE', 'fi_FI', 'fr_XX', 'gu_IN', 'hi_IN', 'it_IT', 'ja_XX', 'kk_KZ', 'ko_KR', 'lt_LT', 'lv_LV', 'my_MM', 'ne_NP', 'nl_XX', 'ro_RO', 'ru_RU', 'si_LK', 'tr_TR', 'vi_VN', 'zh_CN', 'af_ZA', 'az_AZ', 'bn_IN', 'fa_IR', 'he_IL', 'hr_HR', 'id_ID', 'ka_GE', 'km_KH', 'mk_MK', 'ml_IN', 'mn_MN', 'mr_IN', 'pl_PL', 'ps_AF', 'pt_XX', 'sv_SE', 'sw_KE', 'ta_IN', 'te_IN', 'th_TH', 'tl_XX', 'uk_UA', 'ur_PK', 'xh_ZA', 'gl_ES', 'sl_SI')\n",
            "{'Arabic': 'ar_AR', 'Czech': 'cs_CZ', 'German': 'de_DE', 'English': 'en_XX', 'Spanish': 'es_XX', 'Estonian': 'et_EE', 'Finnish': 'fi_FI', 'French': 'fr_XX', 'Gujarati': 'gu_IN', 'Hindi': 'hi_IN', 'Italian': 'it_IT', 'Japanese': 'ja_XX', 'Kazakh': 'kk_KZ', 'Korean': 'ko_KR', 'Lithuanian': 'lt_LT', 'Latvian': 'lv_LV', 'Burmese': 'my_MM', 'Nepali': 'ne_NP', 'Dutch': 'nl_XX', 'Romanian': 'ro_RO', 'Russian': 'ru_RU', 'Sinhala': 'si_LK', 'Turkish': 'tr_TR', 'Vietnamese': 'vi_VN', 'Chinese': 'zh_CN', 'Afrikaans': 'af_ZA', 'Azerbaijani': 'az_AZ', 'Bengali': 'bn_IN', 'Persian': 'fa_IR', 'Hebrew': 'he_IL', 'Croatian': 'hr_HR', 'Indonesian': 'id_ID', 'Georgian': 'ka_GE', 'Khmer': 'km_KH', 'Macedonian': 'mk_MK', 'Malayalam': 'ml_IN', 'Mongolian': 'mn_MN', 'Marathi': 'mr_IN', 'Polish': 'pl_PL', 'Pashto': 'ps_AF', 'Portuguese': 'pt_XX', 'Swedish': 'sv_SE', 'Swahili': 'sw_KE', 'Tamil': 'ta_IN', 'Telugu': 'te_IN', 'Thai': 'th_TH', 'Tagalog': 'tl_XX', 'Ukrainian': 'uk_UA', 'Urdu': 'ur_PK', 'Xhosa': 'xh_ZA', 'Galician': 'gl_ES', 'Slovene': 'sl_SI'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RFN5AplfYIiM"
      },
      "source": [
        "## Advanced\n",
        "\n",
        "The following section assumes you have knowledge of PyTorch and Huggingface Transformers.\n",
        "\n",
        "### Saving and loading\n",
        "\n",
        "If you wish to accelerate the loading time the translation model, you can use `save_obj`:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "UaDQVFlzYIiN"
      },
      "source": [
        "mt.save_obj(\"saved_model\")"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c_uMZ9exYIiN"
      },
      "source": [
        "\n",
        "Then later you can reload it with `load_obj`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SEkmXUDaYIiN",
        "outputId": "823d561c-c949-4734-8f50-dd308dc6b956"
      },
      "source": [
        "%%time\n",
        "mt = dlt.TranslationModel.load_obj('saved_model')"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 1.96 s, sys: 1.94 s, total: 3.9 s\n",
            "Wall time: 4.69 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j4G9tRL8YIiO"
      },
      "source": [
        "\n",
        "**Warning:** Only use this if you are certain the torch module saved in `saved_model/weights.pt` can be correctly loaded. Indeed, it is possible that the `huggingface`, `torch` or some other dependencies change between when you called `save_obj` and `load_obj`, and that might break your code. Thus, it is recommend to only run `load_obj` in the same environment/session as `save_obj`. **Note this method might be deprecated in the future once there's no speed benefit in loading this way.**\n",
        "\n",
        "\n",
        "### Interacting with underlying model and tokenizer\n",
        "\n",
        "When initializing `model`, you can pass in arguments for the underlying BART model and tokenizer (which will respectively be passed to `MBartForConditionalGeneration.from_pretrained` and `MBart50TokenizerFast.from_pretrained`):\n",
        "\n",
        "```python\n",
        "mt = dlt.TranslationModel(\n",
        "    model_options=dict(\n",
        "        state_dict=...,\n",
        "        cache_dir=...,\n",
        "        ...\n",
        "    ),\n",
        "    tokenizer_options=dict(\n",
        "        tokenizer_file=...,\n",
        "        eos_token=...,\n",
        "        ...\n",
        "    )\n",
        ")\n",
        "```\n",
        "\n",
        "You can also access the underlying `transformers` model and `tokenizer`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_kg_hide-output": true,
        "trusted": true,
        "id": "UX1lyl_uYIiO"
      },
      "source": [
        "bart = mt.get_transformers_model()\n",
        "tokenizer = mt.get_tokenizer()\n",
        "\n",
        "print(tokenizer)\n",
        "print(bart)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQPqtEeuYIiO"
      },
      "source": [
        "See the [huggingface docs](https://huggingface.co/transformers/master/model_doc/mbart.html) for more information.\n",
        "\n",
        "\n",
        "### `bart_model.generate()` keyword arguments\n",
        "\n",
        "When running `mt.translate`, you can also give a `generation_options` dictionary that is passed as keyword arguments to the underlying `bart_model.generate()` method:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XuTbvJBWYIiP",
        "outputId": "8f202ef9-d338-4630-9b58-04d1d957cc89"
      },
      "source": [
        "mt.translate(\n",
        "    sents,\n",
        "    source=dlt.lang.ENGLISH,\n",
        "    target=dlt.lang.SPANISH,\n",
        "    generation_options=dict(num_beams=5, max_length=128)\n",
        ")"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['El Sr. Smith fue a su caf√© favorito.',\n",
              " 'En ese lugar, se reuni√≥ con su amigo Dr. Doe.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QvwPa2b1YIiP"
      },
      "source": [
        "Learn more in the [huggingface docs](https://huggingface.co/transformers/main_classes/model.html#transformers.generation_utils.GenerationMixin.generate).\n",
        "\n",
        "\n",
        "## Acknowledgement\n",
        "\n",
        "`dl-translate` is built on top of Huggingface's implementation of multilingual BART finetuned on many-to-many translation of over 50 languages, which is [documented here](https://huggingface.co/transformers/master/model_doc/mbart.html). The original paper was written by Tang et. al from Facebook AI Research; you can [find it here](https://arxiv.org/pdf/2008.00401.pdf) and cite it using the following:\n",
        "```\n",
        "@article{tang2020multilingual,\n",
        "  title={Multilingual translation with extensible multilingual pretraining and finetuning},\n",
        "  author={Tang, Yuqing and Tran, Chau and Li, Xian and Chen, Peng-Jen and Goyal, Naman and Chaudhary, Vishrav and Gu, Jiatao and Fan, Angela},\n",
        "  journal={arXiv preprint arXiv:2008.00401},\n",
        "  year={2020}\n",
        "}\n",
        "```\n",
        "\n",
        "`dlt` is a wrapper with useful `utils` to save you time. For huggingface's `transformers`, the following snippet is shown as an example:\n",
        "```python\n",
        "from transformers import MBartForConditionalGeneration, MBart50TokenizerFast\n",
        "\n",
        "article_hi = \"‡§∏‡§Ç‡§Ø‡•Å‡§ï‡•ç‡§§ ‡§∞‡§æ‡§∑‡•ç‡§ü‡•ç‡§∞ ‡§ï‡•á ‡§™‡•ç‡§∞‡§Æ‡•Å‡§ñ ‡§ï‡§æ ‡§ï‡§π‡§®‡§æ ‡§π‡•à ‡§ï‡§ø ‡§∏‡•Ä‡§∞‡§ø‡§Ø‡§æ ‡§Æ‡•á‡§Ç ‡§ï‡•ã‡§à ‡§∏‡•à‡§®‡•ç‡§Ø ‡§∏‡§Æ‡§æ‡§ß‡§æ‡§® ‡§®‡§π‡•Ä‡§Ç ‡§π‡•à\"\n",
        "article_ar = \"ÿßŸÑÿ£ŸÖŸäŸÜ ÿßŸÑÿπÿßŸÖ ŸÑŸÑÿ£ŸÖŸÖ ÿßŸÑŸÖÿ™ÿ≠ÿØÿ© ŸäŸÇŸàŸÑ ÿ•ŸÜŸá ŸÑÿß ŸäŸàÿ¨ÿØ ÿ≠ŸÑ ÿπÿ≥ŸÉÿ±Ÿä ŸÅŸä ÿ≥Ÿàÿ±Ÿäÿß.\"\n",
        "\n",
        "model = MBartForConditionalGeneration.from_pretrained(\"facebook/mbart-large-50-many-to-many-mmt\")\n",
        "tokenizer = MBart50TokenizerFast.from_pretrained(\"facebook/mbart-large-50-many-to-many-mmt\")\n",
        "\n",
        "# translate Hindi to French\n",
        "tokenizer.src_lang = \"hi_IN\"\n",
        "encoded_hi = tokenizer(article_hi, return_tensors=\"pt\")\n",
        "generated_tokens = model.generate(**encoded_hi, forced_bos_token_id=tokenizer.lang_code_to_id[\"fr_XX\"])\n",
        "tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n",
        "# => \"Le chef de l 'ONU affirme qu 'il n 'y a pas de solution militaire en Syria.\"\n",
        "\n",
        "# translate Arabic to English\n",
        "tokenizer.src_lang = \"ar_AR\"\n",
        "encoded_ar = tokenizer(article_ar, return_tensors=\"pt\")\n",
        "generated_tokens = model.generate(**encoded_ar, forced_bos_token_id=tokenizer.lang_code_to_id[\"en_XX\"])\n",
        "tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n",
        "# => \"The Secretary-General of the United Nations says there is no military solution in Syria.\"\n",
        "```\n",
        "\n",
        "With `dlt`, you can run:\n",
        "```python\n",
        "import dl_translate as dlt\n",
        "\n",
        "article_hi = \"‡§∏‡§Ç‡§Ø‡•Å‡§ï‡•ç‡§§ ‡§∞‡§æ‡§∑‡•ç‡§ü‡•ç‡§∞ ‡§ï‡•á ‡§™‡•ç‡§∞‡§Æ‡•Å‡§ñ ‡§ï‡§æ ‡§ï‡§π‡§®‡§æ ‡§π‡•à ‡§ï‡§ø ‡§∏‡•Ä‡§∞‡§ø‡§Ø‡§æ ‡§Æ‡•á‡§Ç ‡§ï‡•ã‡§à ‡§∏‡•à‡§®‡•ç‡§Ø ‡§∏‡§Æ‡§æ‡§ß‡§æ‡§® ‡§®‡§π‡•Ä‡§Ç ‡§π‡•à\"\n",
        "article_ar = \"ÿßŸÑÿ£ŸÖŸäŸÜ ÿßŸÑÿπÿßŸÖ ŸÑŸÑÿ£ŸÖŸÖ ÿßŸÑŸÖÿ™ÿ≠ÿØÿ© ŸäŸÇŸàŸÑ ÿ•ŸÜŸá ŸÑÿß ŸäŸàÿ¨ÿØ ÿ≠ŸÑ ÿπÿ≥ŸÉÿ±Ÿä ŸÅŸä ÿ≥Ÿàÿ±Ÿäÿß.\"\n",
        "\n",
        "mt = dlt.TranslationModel()\n",
        "translated_fr = mt.translate(article_hi, source=dlt.lang.HINDI, target=dlt.lang.FRENCH)\n",
        "translated_en = mt.translate(article_ar, source=dlt.lang.ARABIC, target=dlt.lang.ENGLISH)\n",
        "```\n",
        "\n",
        "Notice you don't have to think about tokenizers, condition generation, pretrained models, and regional codes; you can just tell the model what to translate!\n",
        "\n",
        "If you are experienced with `huggingface`'s ecosystem, then you should be familiar enough with the example above that you wouldn't need this library. However, if you've never heard of huggingface or mBART, then I hope using this library will give you enough motivation to [learn more about them](https://github.com/huggingface/transformers) :)"
      ]
    }
  ]
}